{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# KubeWise – Kubernetes Resource Agent\n\nThis notebook implements the KubeWise agent for the Agents Intensive – Capstone Project (Enterprise Agents track).","metadata":{}},{"cell_type":"code","source":"import os\nimport textwrap\nfrom typing import Dict, List\n\nimport numpy as np\nimport yaml\n\nfrom google.genai.types import Part, Content\n\nfrom google.adk.agents import LlmAgent, SequentialAgent\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.memory import InMemoryMemoryService\nfrom google.adk.runners import Runner\nfrom google.adk.agents.run_config import RunConfig\nfrom google.adk.tools import load_memory  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:17:47.630206Z","iopub.execute_input":"2025-11-16T19:17:47.630546Z","iopub.status.idle":"2025-11-16T19:17:47.636511Z","shell.execute_reply.started":"2025-11-16T19:17:47.630522Z","shell.execute_reply":"2025-11-16T19:17:47.635456Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\nos.environ[\"GOOGLE_API_KEY\"] = api_key\n\nprint(\"✅ Loaded GOOGLE_API_KEY from Kaggle Secrets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:17:50.840572Z","iopub.execute_input":"2025-11-16T19:17:50.840885Z","iopub.status.idle":"2025-11-16T19:17:50.881375Z","shell.execute_reply.started":"2025-11-16T19:17:50.840864Z","shell.execute_reply":"2025-11-16T19:17:50.880281Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded GOOGLE_API_KEY from Kaggle Secrets.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"MODEL_NAME = \"gemini-2.0-flash\"\nAPP_NAME = \"kubewise-k8s-resource-agent\"\nUSER_ID = \"kubewise-user\"\nSESSION_ID = \"kubewise-session-1\"\n\nos.environ.setdefault(\"GOOGLE_GENAI_USE_VERTEXAI\", \"FALSE\")\n\nif \"GOOGLE_API_KEY\" not in os.environ:\n    print(\n        \"⚠️ GOOGLE_API_KEY is NOT set.\\n\"\n        \"   In Kaggle, add it as a secret named GOOGLE_API_KEY \"\n        \"or export it locally before running the notebook.\"\n    )\nelse:\n    print(\"✅ GOOGLE_API_KEY found in environment.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:17:54.398386Z","iopub.execute_input":"2025-11-16T19:17:54.398694Z","iopub.status.idle":"2025-11-16T19:17:54.405571Z","shell.execute_reply.started":"2025-11-16T19:17:54.398673Z","shell.execute_reply":"2025-11-16T19:17:54.404203Z"}},"outputs":[{"name":"stdout","text":"✅ GOOGLE_API_KEY found in environment.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"SYNTHETIC_METRICS: Dict[str, Dict[str, List[float]]] = {\n    \"prod/orders-service\": {\n        \"cpu_mcores\": [80, 120, 90, 140, 200, 160, 130, 150, 180, 210],\n        \"memory_mib\": [220, 240, 230, 260, 280, 300, 310, 295, 305, 320],\n    },\n    \"prod/payments-service\": {\n        \"cpu_mcores\": [40, 60, 50, 55, 70, 65, 75, 60, 80, 90],\n        \"memory_mib\": [150, 160, 155, 170, 165, 175, 180, 190, 185, 200],\n    },\n    \"nonprod/reporting-service\": {\n        \"cpu_mcores\": [20, 30, 25, 35, 40, 28, 32, 36, 38, 45],\n        \"memory_mib\": [100, 110, 105, 120, 130, 115, 118, 122, 125, 135],\n    },\n}\n\n\ndef _metrics_key(namespace: str, workload: str) -> str:\n    return f\"{namespace}/{workload}\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:17:57.978728Z","iopub.execute_input":"2025-11-16T19:17:57.979120Z","iopub.status.idle":"2025-11-16T19:17:57.987053Z","shell.execute_reply.started":"2025-11-16T19:17:57.979094Z","shell.execute_reply":"2025-11-16T19:17:57.985643Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def load_usage_samples(namespace: str, workload: str) -> Dict[str, object]:\n    key = _metrics_key(namespace, workload)\n\n    if key not in SYNTHETIC_METRICS:\n        np.random.seed(abs(hash(key)) % (2**32))\n        cpu = np.random.normal(loc=150, scale=40, size=60).clip(20)\n        mem = np.random.normal(loc=300, scale=80, size=60).clip(64)\n        SYNTHETIC_METRICS[key] = {\n            \"cpu_mcores\": cpu.tolist(),\n            \"memory_mib\": mem.tolist(),\n        }\n\n    data = SYNTHETIC_METRICS[key]\n    return {\n        \"namespace\": namespace,\n        \"workload\": workload,\n        \"cpu_mcores\": data[\"cpu_mcores\"],\n        \"memory_mib\": data[\"memory_mib\"],\n    }\n\n\ndef summarize_usage(namespace: str, workload: str) -> Dict[str, float]:\n    samples = load_usage_samples(namespace, workload)\n    cpu = np.array(samples[\"cpu_mcores\"])\n    mem = np.array(samples[\"memory_mib\"])\n\n    summary = {\n        \"namespace\": namespace,\n        \"workload\": workload,\n        \"cpu_avg_mcores\": float(cpu.mean()),\n        \"cpu_p95_mcores\": float(np.percentile(cpu, 95)),\n        \"cpu_max_mcores\": float(cpu.max()),\n        \"memory_avg_mib\": float(mem.mean()),\n        \"memory_p95_mib\": float(np.percentile(mem, 95)),\n        \"memory_max_mib\": float(mem.max()),\n        \"num_samples\": len(cpu),\n    }\n    return summary\n\n\ndef generate_resource_patch(\n    namespace: str,\n    workload: str,\n    current_yaml: str,\n    target_utilization: float = 0.7,\n) -> Dict[str, object]:\n\n    metrics = summarize_usage(namespace, workload)\n\n    obj = yaml.safe_load(current_yaml)\n\n    container = obj[\"spec\"][\"template\"][\"spec\"][\"containers\"][0]\n    resources = container.setdefault(\"resources\", {})\n    requests = resources.setdefault(\"requests\", {})\n    limits = resources.setdefault(\"limits\", {})\n\n    cpu_p95 = metrics[\"cpu_p95_mcores\"]\n    mem_p95 = metrics[\"memory_p95_mib\"]\n\n    cpu_request = max(25, int(cpu_p95 / target_utilization))\n    cpu_limit = int(cpu_request * 1.5)\n\n    mem_request = max(64, int(mem_p95 / target_utilization))\n    mem_limit = int(mem_request * 1.4)\n\n    requests[\"cpu\"] = f\"{cpu_request}m\"\n    requests[\"memory\"] = f\"{mem_request}Mi\"\n    limits[\"cpu\"] = f\"{cpu_limit}m\"\n    limits[\"memory\"] = f\"{mem_limit}Mi\"\n\n    patched_yaml = yaml.safe_dump(obj, sort_keys=False)\n\n    return {\n        \"namespace\": namespace,\n        \"workload\": workload,\n        \"target_utilization\": target_utilization,\n        \"metrics_summary\": metrics,\n        \"suggested_requests\": {\n            \"cpu_mcores\": cpu_request,\n            \"memory_mib\": mem_request,\n        },\n        \"suggested_limits\": {\n            \"cpu_mcores\": cpu_limit,\n            \"memory_mib\": mem_limit,\n        },\n        \"patched_yaml\": patched_yaml,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:18:01.397320Z","iopub.execute_input":"2025-11-16T19:18:01.397744Z","iopub.status.idle":"2025-11-16T19:18:01.412486Z","shell.execute_reply.started":"2025-11-16T19:18:01.397715Z","shell.execute_reply":"2025-11-16T19:18:01.411245Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"metrics_agent = LlmAgent(\n    name=\"MetricsAnalystAgent\",\n    model=MODEL_NAME,\n    description=\"Analyzes Kubernetes CPU & memory usage for a single workload.\",\n    instruction=textwrap.dedent(\"\"\"\n        You are a Kubernetes SRE and capacity planner.\n        For the given namespace and workload:\n        1. Call the `load_usage_samples` tool to retrieve CPU/memory samples.\n        2. Optionally call `summarize_usage` to get summary statistics.\n        3. Produce a compact summary that includes avg, p95 and max\n           CPU (mcores) and memory (MiB), plus one paragraph of interpretation.\n        Return your analysis as clear Markdown.\n    \"\"\").strip(),\n    tools=[load_usage_samples, summarize_usage],\n    output_key=\"metrics_summary\"  \n)\n\nplanner_agent = LlmAgent(\n    name=\"ResourcePlannerAgent\",\n    model=MODEL_NAME,\n    description=\"Maps usage metrics to Kubernetes resource requests & limits.\",\n    instruction=textwrap.dedent(\"\"\"\n        You are a Kubernetes performance engineer.\n        You receive:\n        - `metrics_summary` in the session state with usage statistics.\n        - A Deployment YAML snippet for the target workload.\n\n        Use the `generate_resource_patch` tool to compute new CPU/memory\n        requests and limits that target around 70% utilization at p95.\n        Then briefly explain why these values are safe and cost-aware.\n    \"\"\").strip(),\n    tools=[generate_resource_patch],\n    output_key=\"tuning_plan\"\n)\n\nexplainer_agent = LlmAgent(\n    name=\"DevOpsExplainerAgent\",\n    model=MODEL_NAME,\n    description=\"Explains tuning changes in friendly language for app owners.\",\n    instruction=textwrap.dedent(\"\"\"\n        You are a friendly DevOps teammate named KubeWise.\n        You receive a `tuning_plan` from the previous agent.\n\n        Tasks:\n        1. First, optionally call the `load_memory` tool to see if there were\n           previous KubeWise recommendations for this workload and highlight trends.\n        2. Produce a clear explanation for an application team including:\n           - Short summary of the issue.\n           - A table-style text with OLD vs NEW CPU/memory requests/limits.\n           - Expected impact on reliability and cloud cost.\n           - A safe rollout plan (e.g., canary, progressive rollout, monitoring).\n\n        Keep the tone professional but supportive.\n    \"\"\").strip(),\n    tools=[load_memory],\n)\n\npipeline_agent = SequentialAgent(\n    name=\"KubeWisePipeline\",\n    description=\"KubeWise pipeline: metrics → resource planning → explanation.\",\n    sub_agents=[metrics_agent, planner_agent, explainer_agent],\n)\n\nroot_agent = pipeline_agent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:18:09.050204Z","iopub.execute_input":"2025-11-16T19:18:09.050684Z","iopub.status.idle":"2025-11-16T19:18:09.060008Z","shell.execute_reply.started":"2025-11-16T19:18:09.050656Z","shell.execute_reply":"2025-11-16T19:18:09.058418Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"\nsession_service = InMemorySessionService()\nmemory_service = InMemoryMemoryService()\n\nawait session_service.create_session(\n    app_name=APP_NAME,\n    user_id=USER_ID,\n    session_id=SESSION_ID,\n)\n\nrunner = Runner(\n    agent=root_agent,\n    app_name=APP_NAME,\n    session_service=session_service,\n    memory_service=memory_service,\n)\n\ndef ask_kubewise(question: str) -> str:\n    user_msg = Content(\n        role=\"user\",\n        parts=[Part.from_text(text=question)],\n    )\n\n    run_config = RunConfig(response_modalities=[\"TEXT\"])\n\n    final_text = \"\"\n    events = runner.run(\n        user_id=USER_ID,\n        session_id=SESSION_ID,\n        new_message=user_msg,\n        run_config=run_config,\n    )\n    for event in events:\n        if event.is_final_response() and event.content and event.content.parts:\n            final_text = event.content.parts[0].text\n\n    return final_text.strip()\n\n\nasync def save_session_to_memory():\n    completed_session = await runner.session_service.get_session(\n        app_name=APP_NAME,\n        user_id=USER_ID,\n        session_id=SESSION_ID,\n    )\n    await memory_service.add_session_to_memory(completed_session)\n    print(\"✅ Session added to in-memory long-term storage.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:18:22.982904Z","iopub.execute_input":"2025-11-16T19:18:22.984177Z","iopub.status.idle":"2025-11-16T19:18:22.993629Z","shell.execute_reply.started":"2025-11-16T19:18:22.984136Z","shell.execute_reply":"2025-11-16T19:18:22.992201Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## End-to-end KubeWise Demo\n\nThe cell below runs the full KubeWise agent pipeline for the `prod/orders-service`\nworkload: it pulls metrics, analyzes them, proposes new CPU/memory resources,\nand explains the impact + rollout strategy.\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", module=\"google.genai.types\")\n\nexample_deployment_yaml = \"\"\"\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-service\n  namespace: prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: orders-service\n  template:\n    metadata:\n      labels:\n        app: orders-service\n    spec:\n      containers:\n      - name: orders\n        image: mycorp/orders:1.0\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"512Mi\"\n\"\"\"\n\nuser_prompt = (\n    \"You are KubeWise, a Kubernetes resource tuning assistant.\\n\\n\"\n    \"Namespace: prod\\n\"\n    \"Workload: orders-service\\n\\n\"\n    \"Here is the current Deployment YAML:\\n\\n\"\n    + example_deployment_yaml\n    + \"\\n\\nPlease:\\n\"\n    \"- Analyze recent CPU and memory usage for this workload.\\n\"\n    \"- Recommend new CPU/memory requests and limits based on the metrics.\\n\"\n    \"- Explain the change, the expected impact on reliability and cost,\\n\"\n    \"  and how to roll out the change safely.\\n\"\n)\n\nresponse = ask_kubewise(user_prompt)\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:32:24.645854Z","iopub.execute_input":"2025-11-16T19:32:24.646679Z","iopub.status.idle":"2025-11-16T19:32:44.233522Z","shell.execute_reply.started":"2025-11-16T19:32:24.646587Z","shell.execute_reply":"2025-11-16T19:32:44.232227Z"}},"outputs":[{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\nWARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Alright, let's break down the updated tuning plan for the `orders-service` in the `prod` namespace. It's great that we're taking a more cautious approach this time! No prior tuning history exists for this workload.\n\n**Summary:**\n\nWe're taking a step-by-step approach to optimize resources for the `orders-service`. Instead of a large jump, we're making smaller, more manageable increases to CPU and memory requests and limits. This allows us to closely monitor the impact and avoid potential disruptions.\n\n**Proposed Changes:**\n\nHere's the resource configuration we're aiming for:\n\n| Resource   | Old Request | New Request | Old Limit | New Limit |\n| ---------- | ----------- | ----------- | --------- | --------- |\n| CPU (mcores) | 100         | 200         | 300       | 350       |\n| Memory (MiB) | 256         | 350         | 512       | 550       |\n\n**Explanation:**\n\n*   **CPU Requests:** Increased to 200m. This aligns with the observed P95 CPU usage (around 205mcores).\n*   **Memory Requests:** Increased to 350Mi. This gives a little headroom above the observed P95 memory usage (around 315MiB).\n*   **CPU Limits:** Increased to 350m. This is a small bump to allow for occasional spikes.\n*   **Memory Limits:** Increased to 550Mi. Just a slight increase to give a bit more breathing room.\n\n**Expected Impact:**\n\n*   **Improved Reliability:** This should still improve reliability compared to the original configuration, but with less risk. We're addressing the undersized requests that could cause problems.\n*   **Cost Awareness:** By making smaller changes, we're keeping a closer eye on costs. We can always increase resources further if needed, but this approach helps us avoid over-provisioning.\n\n**Rollout Strategy:**\n\nHere's the plan for a safe rollout:\n\n1.  **Apply the YAML:** Use `kubectl apply -f deployment.yaml` to update the deployment.\n2.  **Monitor Closely:** This is the most important part! After applying the changes, keep a close watch on:\n    *   **CPU and Memory Utilization:** Are we seeing better utilization without maxing out the new limits?\n    *   **Request Latency and Error Rates:** Are there any performance regressions?\n    *   **Throttling and OOMKills:** Are we still seeing any resource-related errors?\n3.  **Iterate and Refine:** After a week or so, analyze the data. If things are stable and utilization is high, we can consider another small increase. The key is to take it slow and learn as we go.\n\nThis gradual approach gives us the best chance of optimizing the `orders-service` resources without causing any surprises. Let me know if you have any questions!\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"## Memory demo: recalling past KubeWise recommendations\n\nThe cells below store the current conversation into long-term memory and then\nask KubeWise to recall its previous tuning advice for `orders-service`.\n","metadata":{}},{"cell_type":"code","source":"await save_session_to_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:37:27.034853Z","iopub.execute_input":"2025-11-16T19:37:27.035509Z","iopub.status.idle":"2025-11-16T19:37:27.043921Z","shell.execute_reply.started":"2025-11-16T19:37:27.035478Z","shell.execute_reply":"2025-11-16T19:37:27.042234Z"}},"outputs":[{"name":"stdout","text":"✅ Session added to in-memory long-term storage.\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"follow_up_prompt = (\n    \"Earlier you generated a tuning plan for the `orders-service` workload \"\n    \"in the `prod` namespace. Using any memory you have stored, briefly \"\n    \"summarize what you previously recommended and why, and explain how an \"\n    \"SRE could use that history to decide the next tuning iteration.\"\n)\n\nfollow_up_response = ask_kubewise(follow_up_prompt)\nprint(follow_up_response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T19:37:43.754916Z","iopub.execute_input":"2025-11-16T19:37:43.755245Z","iopub.status.idle":"2025-11-16T19:38:03.499578Z","shell.execute_reply.started":"2025-11-16T19:37:43.755221Z","shell.execute_reply":"2025-11-16T19:38:03.498358Z"}},"outputs":[{"name":"stderr","text":"WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n","output_type":"stream"},{"name":"stdout","text":"Okay, I can definitely summarize the previous recommendations and explain how an SRE can use that history.\n\n**Previous Recommendations (Brief Summary):**\n\nInitially, the `orders-service` had very low resource requests (100m CPU, 256Mi memory) relative to its observed usage.\n\n*   **First Attempt (Too Aggressive):** The first tuning plan suggested a large increase targeting 70% utilization at the 95th percentile. This meant bumping the requests up to 293m CPU and 450Mi memory. The goal was improved reliability, but the jump was considered too risky.\n*   **Second Attempt (More Conservative):** The second plan scaled back the changes, aiming for a more gradual approach. Requests were increased to 200m CPU and 350Mi memory, aligning more closely with the observed P95 usage. The emphasis was on minimizing risk and allowing for better monitoring before making further adjustments.\n\n**How an SRE Can Use This History for the Next Iteration:**\n\nThe SRE should use the prior attempts as lessons learned to inform the next iteration:\n\n1.  **Monitor the Current Deployment:** The most important step is to closely monitor the `orders-service` after applying the *second* (more conservative) set of changes. The SRE needs to track CPU and memory utilization (average, peak, and 95th percentile), request latency, error rates, and any signs of throttling or OOMKills.\n\n2.  **Evaluate Against Original Goals:** Is the `orders-service` more reliable now? Are we seeing fewer resource-related issues? Is the service performing consistently?\n\n3.  **Analyze Utilization:**\n    *   *High Utilization (approaching the new limits):* This suggests that we might need to increase resources further.\n    *   *Low Utilization:* This suggests that the current configuration is sufficient, or perhaps even over-provisioned, and we could potentially scale down.\n\n4.  **Learn from Past Mistakes:** We already know that large, aggressive changes can be risky. Stick to the principle of gradual, data-driven adjustments.\n\n5.  **Specific Scenarios & Actions:**\n    *   *Stable Performance, Low Utilization:* Maintain the current configuration and continue monitoring. Consider reducing resources if the low utilization persists.\n    *   *Stable Performance, High Utilization:* Consider another *small* increase in resources, but monitor *very* closely.\n    *   *Performance Issues Persist (Throttling, OOMKills):* Investigate further! The problem might not be solely resource-related. Check application logs and consider profiling the application. Increase resources *cautiously* and in small increments.\n\n6.  **Keep a Tuning Log:** Document *everything* – each change, the rationale behind it, and the observed impact. This log will be invaluable for future tuning efforts.\n\nIn short, the SRE should prioritize a data-driven, iterative, and *cautious* approach. The goal is to find the right balance between reliability and cost-efficiency, and the tuning history provides valuable context for making informed decisions.\n","output_type":"stream"}],"execution_count":56}]}